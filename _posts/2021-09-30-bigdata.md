---
title: Big Data
---

Alright, let’s talk about **Big Data**. What's Big Data? It's working with very very large amounts of data. That means processing it, storing it, querying it and using it to do analysis. This blog post is going to be focused on concepts, frameworks, tools, and databases that are built for big data processing and storage, for analytical use cases. If you’re looking for a more general introduction to databases, I’d recommend you start with the [Databases]({% post_url 2021-09-23-databases %}) post and then jump back into this post. 

You may have heard technologies like **Hive**. Or maybe **HBase**. But when you go to google it, you get results like: 
  
```Apache Hive is a distributed, fault-tolerant data warehouse system that enables analytics at a massive scale``` 

  And 
  
```HBase is an open source, non-relational, distributed database modeled after Google's BigTable and is written in Java```. 
  
Unless you’re already basically an expert in the field that’s basically gibberish. Except for maybe that it’s "written in java" part. Grrreat. Ok what about the next sentence? 
  
```It is developed as part of Apache Software Foundation's Apache Hadoop project and runs on top of HDFS (Hadoop Distributed Filesystem), providing BigTable-like capabilities for Hadoop.``` 

As you can see, you basically have to already know what HBase is to understand this definition of HBase. Or at least you have to know what *BigTable, HDFS, Hadoop* and *Apache* all are. If you’re googling to understand what Hive actually is, that explanation doesn't help much. 

You’ll come across a lottt of terminology and technologies in the world of Big Data. Anything you read with be peppered with terms and names, that unless you have a good footing under you, it can be all too easy to throw your hands in the air and just wish that jargon won’t be described with more jargon.  

Let's zoom back and get familiar with the *big picture* so you can put new technologies that you come across into context.

----
****
  
**50,000 foot view:** Introduction to analytical processing and use cases. 

**Bird's eye view:** High level concepts around what it means to work with Big Data. 

**On the ground:** Industry standard big data processing frameworks and data warehouses

----
****

## 50,000 foot view
Big Data, plain and simple, is working with A LOT of data. "working with" means storing, processing, querying and visualizing. 

### Transactional vs Analytical

The [databases]({% post_url 2021-09-23-databases %}) blog post was primarily focused on **transactional** use cases, whereas this one will be primarily focused on **analytical** use cases. 

**Transactional databases** are ones where you’re serving a customer in real time. For example, like showing a user’s *facebook wall* or *amazon cart* or even *google search results*. Here, it's a large amount of data *and* performance is really important, so you need databases that can retrieve data in a matter of milliseconds. And you’ll need your databases to be designed such that you can identify the needle in the haystack, so to speak. You’ll have to deal with a lot of data but the focus is on finding the correct information to serve to the front end. This is one aspect of *Big Data*. One which necessitated creation of NoSQL databases.

*This* blog post will focus on Big Data as it applies to **analytical** use cases. For this you have a lot of data, and you’re trying to draw some *insights* based on this. It could be a business that’s looking to find some useful information or patterns out of it. Like, seeing the top ten most popular items on amazon today, for the month, and for the year, grouped by the country or globally. 
  
For analytical use-cases, you're processing a lot of data. It's not so much finding a needle in a haystack as combing through the haystack to form a pile of hay of a particular color. Your focus is going to be on how to *optimize* all this processing, and considering all the factors that goes into it. Like, how many *workers/machines* to allocate to parallelize, which *processing framework and algorithms* to use, where to *store* this data, *cost/benefit* analysis of ultra powerful machines. *This* is **Big Data**.
  
And then, even after it is processed in some form, you may still have an extremely large amount of data- like the number of people who watched each show on netflix for each day. And you’ll want to store this processed information in a database that can be used by *data-scientists* or *machine learning experts* to draw further insights and recommendations based on this data. Or you can can use this to *show to a user*. Like, showing analytics for a certain show to the staff/producers for that show. 
  
And this here - *showing these analytics to an end-user* - is one place where there's *overlap* between transactional and analytical use cases. You’re showing analytics information to a real consumer so sub-second performance is important on a large amount of data. *Column-oriented databases* will come in handy here. 

And yet another overlap is in tuning how many machines to use to store your data and how to spread data across those machines. That's a factor in transactional cases too when you're deciding how to shard and scale NoSQL databases horizontally. 

----
****

## Bird's eye view

Let’s dig a little deeper and highlight the big components of Big Data.

### Data Storage vs. Data Processing

There are really **two high level components** when it comes to Big Data. That’s **data storage** and **data processing**. 
  
Here’s just some of the technologies you’ll come across, and you may have seen or heard of some of them already - **Hadoop, Hive, MapReduce, Redshift, Spark, Presto, HBase, Pig, HDFS, Yarn, Pinot, Tez, Mesos, R, Ambari, Mahout, Zookeeper, Flink, Flume, etc, etc, etc**. The very first thing you’ll want to recognize when you come across any of this is - is this for **storing data** or **processing data**? Those are the two big components of Big Data. Now don’t get me wrong, within each there’s lots of sub-categories, but that’s a great starting point for you, because the descriptions you come across on the internet won’t help much unless you have this understanding for yourself. 
  
Lastly, there’s also a big caveat here - keep in mind though that processing and storage aren’t mutually exclusive. For example, a database would go in the **storage** category. BUT, when you execute a query on the database, there could be some amount of *processing* that occurs in order for it to search the storage and find you the response. A lot of times that’s tied together - like the database storage will come with the server that does the processing that needs to happen to access that data. But we’ll also see cases where the processing to access that data is a *separate* technology. Keep this in mind. **Presto** is an example of this, which we'll look into later. It's referred to as a **query engine**.


### End-to-end Example
Let’s take a concrete end-to-end example using **Netflix**. 

1. There could be a *log* dedicated to users watching Netflix. One log file could contain 1000 lines, with each line containing user_id, show_id, episode_id, watch_duration, start_time, end_time, device_type, etc. And there could be hundreds of thousands of these files per day. **Storage**.

2. Then you would create pipelines consuming this large dataset and performing parallel processing and operations on it to join with other datasets and *aggregate it at a higher level*, to get stats on like how many times each show was watched per day, or how many shows are watched during different times of the day, or what genre of shows were most popular. **Processing**.

3. These higher-level aggregations would then be stored in a different place - that could be another set of files or a column-oriented database, or a wide-column store - depending on the use-case. **Storage**.

4. At that point, this dataset could be fed into a machine learning algorithm to suggest new shows for users, or it could be used by data scientists, analysts and marketers to visualize through different tools and determine what new shows to create or what further insights would be useful or it could be used by the financial teams to figure out compensation for the shows. **Processing**.

As you can see Big Data is a lot of high level components of storing, processing, then storing again, then processing again. Each processing step is book-ended by storage steps - which makes sense - you want to read from somewhere and write to somewhere. Each storage step is also book-ended by processing steps - you have to have some workflow to get it into storage, and once it’s there you want to use it somewhere or else it’s just sitting there not being useful. 

>Some light **terminology** 
>>When you’re on the processing data, the storage that you read from is called the **source** and the one you write to is called a **sink**. And when you’re processing data (to turn from raw logs into aggregated data, for example), this can be done with **batch processing** or **stream processing**. Batch processing is scheduled and happens at certain intervals. You can process each day’s data on a daily basis. Or that can be weekly, or hourly, what have you. Stream processing is what it sounds like - the data continuously flows in.

#### Data Lakes vs Data Warehouse
The storage that contains the raw log data is called **Data Lake**. It's almost always a **cloud file storage**, from my experience. That’s because this is a lot of data. Usually *orders of magnitude* larger than aggregated data. And cloud file systems are usually the cheapest, most reliable way to store it. 

File systems aren’t super fast to read from but they don’t need to be. You’re only using this for processing it into aggregated data that you can make more sense of. You're generally not querying for raw-log data from anywhere that needs high-speed performance. 

Then the place where you store the aggregated data is called **Data Warehouse**. This can be either a cloud file system again, or a database. Figuring out what this should be depends on what the data will be used for.  Is the data in storage going to be used by data scientists to analyze the data, or by a realtime application? 

If it’s for a realtime application then it needs to be optimized for high-speed performance and consistency- it can be a column-oriented database like **Redshift**. If it’s data scientists looking to extract meaning from the aggregated data, then the data warehouse could be something that doesn't need to be optimized for high-speed performance, but perhaps should be optimized for consistency and flexibility of the types of queries that can be executed- it could be a serverless database like like **BigQuery**. Data scientists can also analyze data straight from data lakes and form their own aggregations to see which aggregations could be interesting to use and share out. 
  
> **What’s a serverless database?** 
>> Well of course it does have a server, it just means that the server is not your concern - you don’t need to worry about any of the infrastructure of how the database is setup, that’s all handled by the company that's offering the database (in BigQuery's case, Google handles it) and you’re primarily just paying based on how much data is being scanned for the queries you run on it, and storage.

So we've gotten a *bird's eye view* by understanding that the two big components of Big Data are storage and processing. What are the sub-categories within this? Well, we're be working with extremely large amounts of data, so some of the support-technologies surrounding Big Data will be ones that *enable* you to reliably work with such large amounts of data. What does that mean? Well, it means there’s tools for supporting storing large amounts of data and and there’s other tools for supporting processing large amounts of data. 

### Support Technologies
Let’s start with support technologies around storing. Often times, when you’re working with many terabytes or petabytes of data, you often store portions of them on different machines - called sharding or partitioning. We’ve talked about that in the [databases](https://waitbutwhat.com/2021/09/23/databases.html#:~:text=all%20that%20relevant.-,Twig%20-%20Scaling,-Where%20relational%20databases) post. This is because it’s unrealistical to just keep using bigger and bigger single machines to have all your data in one place - the machines will start getting really expensive. If you shard them across many machines, you can use any “commodity” machine that can handle a small/medium amount of data. 

You also may want to replicate the data, or make copies of them in different machines, so that in case something happens to one of the copies, the other one can be used, like a backup. Or actually all the copies can be used simultaneously to give your application better performance and parallelization. If you have 12 requests coming in per second and 3 copies of the data, each copy can handle four requests each. This all sounds great, but the important part of it is that you need another tool/service that can manage and coordinate all of this machines so they can behave like a coherent unit. 
  
Arguably the most popular tool for this is **Zookeeper**, so we’ll refer to it directly. Zookeeper will keep track of which data is in which shard so that it can act like a corrdinator in many scenarios: like when there’s a request coming in, Zookeeper will know which shard to direct that request to, or if one of the machines goes down and becomes unavailable then Zookeeper to do the necessary actions to keep the system in a good state and make sure the replicas are acting like the backup. Or if we need to scale the system even more then the Zookeeper can be told that hey, we want to add four more machines to store the data across - can you rebalance the data across the new machines too? 

Then on the processing side, you have support tools like **YARN**. It stands for *Yet Another Resource Negotiator*. That suggests that there’s a lot of resource negotiators, and there are like Mesos- will let you build that leaf and focus on YARN here. YARN lets you process data that’s on the data lake (or data warehouse, I guess). YARN is similar to Zookeeper in the sense that it is a coordinator and keeps track of the different components doing the actual work. It has the power to manage CPU and memory of the machines that are doing the processing to determine how much of these compute powers go to different processes that want it. Now that I myself read more about it, I think it’s worth dedicating a whole mini blog post to all these resource managers. Because though there’s a lot of technologies specific to data processing resource management - like YARN, Mesos, Flink and so on- there’s also Zookeeper which is resource management for services, then there’s technologies like Kubernetes and Docker which is resource management for applications, and Puppet, Ansible, Terraform and Chef which are configuration management. All of these in my mind fall somewhat into the same category of support technologies that revolve around the main stuff but are kind of key in keeping things functioning properly and coordinating and managing the core technologies. Alright, now that we’ve done the intro and packed up the related-but-can’t-go-into-detail-here-topics, let’s dive into the main technologies!

----
****

## On the ground
When it comes to exploring Big Data in detail, I find that it’s useful to start talking about the actual technologies that are out there to get a sense for this area. There’s a lot of different technologies out there, and if you’re in this area, you’re expected to be familiar with them. I’ll also start by saying that we laid out the two big components of Big Data as storage and processing - we’ll see here that how these two are intelinked, how some of them got created together and work inextricably with each other, and on the other hand how they can be mix and matched. And we’ll talk about the core storage and/or processing technologies and the whole ecosystem that can get built around it that builds up the value of the core technologies. We’ll talk about why there are so many of these, what the differences are and what value each one provides. TBH I’m learning a lot while creating this post, so we’ll see if by the end we can draw some overarching insights from our learnings. 

We’ll start with Hadoop. As far as I can tell, Hadoop is one of the first technologies that was invented that essentially rbought about this field of Big Data. From there, the industry is going through a renaissance and there’s so many tools and technologies that have come up doing similar things and doing things around it.

### Hadoop - Storage+Processing
It’s a framework to store and process big data across many machines. It includes both the storage and processing. There are two components: **HDFS**, which is another cloud file system that we’ve discussed previously and **MapReduce**, which is a programming model for processing large data in parallel across a lot of machines. 

  
> **What do we mean when we say framework or programming model?**
>> Wikipedia actually provided a pretty good one-liner here: “a framework is “an abstraction in which software, providing generic functionality, can be selectively changed by additional user-written code, thus providing application-specific software… the overall program’s flow of control is dictated by the framework”. I thnk of it almost like an abstract class or an interface, which has information like 
  
MapReduce basically dictates that you process data with 2 simple functions: **map()** and **reduce()**. You map your input data to many subsets that can each be processed in parallel. Then you reduce all the processed data and consolidate it. Essentially, Hadoop dictates that you store data in HDFS, and that MapReduce be used to do two things: split up the data in the way that you specify, and consolidate the data in the way that you specify. Now that’s all Hadoop actually is- a file system storage for storing large amounts of data and a framework that specifies how to process that large amount of data. Now we know concretely what Hadoop is when we read the wikipedia definition that it’s a “collection of open-source software utilitiies that facilitates using a network of many computers to solve problems using massive amounts of data and computation”

**A little bit of hadoop history**
History of Hadoop and MapReduce reads like a story so let’s get to it!
  
In 2003 Google invented GFS or Google File System to store a large amount of large files created by their web crawing process in an effort to create Google.

In 2004 Google created MapReduce to process these large amounts of data, and made both GFS and MapReduce open source, called Nutch.

In 2005 the creators of Nutch saw that it was limited in terms of scaling decided to found Yahoo to work on this.

In 2006, they separated the distributed computing parts of Nutch and called it Hadoop, which would work well on thousands of nodes.

In 2008 Hadoop was released as an open source project to Apache

In 2011 Apache released Hadoop 1.0

We talked about what Hadoop is. Then you have all these add-on features and tools that enhance Hadoop and make it more appealing to adopt and more comprehensive. What are those?

### Hive - Processing
I’ve been reading that Hive is a “data warehouse system” or “data warehouse infrastructure software”. What does that mean? Is it actually a data warehouse? No, not exactly. It’s a software that’s an *abstraction of Hadoop’s MapReduce* that provides a *SQL-like interface* to interact with the data that’s in storage. Since it runs on top of Hadoop, it’s all packaged together and considered an entire system that contains multiple things, in broad strokes: 
  
1. the interface for the user to interact with the data using a SQL-like query language called HQL which stands for Hive Query Language.  
  
2. the storage of the data in either HDFS, or a few other places like HBase (? which we’ll get to) and 
  
3. It contains an “engine” that takes in that query and converts it a MapReduce job which is actually the process that gets you the response to your query. 
  
As you can see, Hive is a layer on top of Hadoop that takes in what the user wants to query (aka #1) and uses Hadoop’s Map-Reduce (aka #3) to query the data in HDFS storage (aka #2) to get the user what they queried for. 

You can imagine that Hive isn’t really something that can be used for OLTP - because it needs to use a big MapReduce job that processes a ton of files in parallel to get a response. These types of jobs could run anywhere from a few seconds to a few minutes to many hours. So what is it used for then? It can be used for ETL jobs. ETL stands for Extract, Transform, Load - and it essentially means to transform raw data into some sort of aggregated data that can be used to draw insights out of. To turn your log files into higher level aggregated files like we talked about earlier. Or it could be used by data-scientists who are trying to make sense of the large amounts of data your company has collected and provide some insight out of them. 

>**offshoot - engines**
>>So for #3 from before, What’s the difference between the execution engine and mapreduce itself? You can think of the execution engine as essentially the computer that’s running the job. (For example java runs inside a Java Virtual Machine that is set up and customized by Java Runtime Environment JRE. The execution engine is the central component of the JVM that makes it run - it reads the code and makes it run). Just like java can only be run in a JVM customized by a JVM, Hive queries can be run on a MapReduce execution engine. This execution engine is in charge of taking HQL and converting it to MapReduce. In a later section, we’ll talk about how you can use other types of execution engines for Hive like Tez or Spark.
Zooming out further, there are different types of engines. Query Engine, Processing Engine, Execution Engine.

>**offshooot - Impala**
>>There’s a technology called Imapala that does something similar to Hive. They are competitors and both provide a SQL-like interface to query data stored in Hadoop clusters. I’m not saying data stored in HDFS because as you’ll see in the next section, HDFS, there’s another storage layer in Hadoop called HBase, and Impala can also query that. One core difference of Imapala and Hive is that Impala generates code at runtime to translate from your SQL-like query to code and Hive does it with an “engine” at compile-time to generate code that runs as a “job” on a bunch of machines. The engines (and therefore the jobs) can be either MapReduce, Tez or Spark. Imapala was introduced circa 2012/2013. Imapala is faster than Hive because it doesn’t need to build on top of MapReduce and can be done with in-memory processing. That apparently means that it can operate on the data from exactly where it is, instead of moving it around. Moving data around is called a “shuffle” because data gets shuffled around. The “reduce” step in map-reduce for example needs to bring a lot of data together to see which data can be consolidated together. This will slow it because it requires you to transmit data across the network which is usually the thing that takes the most time. Second to that is accessing the disk. And Hive using MapReduce would do both.

Going back to and finishing up Hive now, Hive was invented by Facebook (ofcourse, they’d want to do analytics on the mountains of data that they have), and later open-sourced to the Apache Foundation. You’ll see that a lot of technologies follow this pattern - of first being invented at a company and then being open-sourced to Apache. We’ll have a different blog post for this.
If Hive is used for ETL and analysis, then what do you use when you want real-time access to data on HDFS to show for an application? Enter HBase…


### HBase - Storage
HBase, similar to Hive, is another thing that *runs on top of Hadoop*. But it’s actually a **column-oriented database** which was built for the use case of doing random reads and writes of the data in HDFS. For example, Pinterest uses 38 HBase clusters which handle 5 million operations per second! So yes, the actual data is *still in HDFS* but HBase has functionality that let’s you *read or write a random piece of data to it*, which is unlike Hadoop which only reads data in HDFS sequentially to process it. 
  
  Because of this, the use-case of HBase is anything that needs fast real-time access to analytics data. Note that real-time doesn’t seem to mean that it can be used for applications or anything. It’s more like it’s for on-demand data analytics, that can be used by a data scientist for example.  So how is HBase column-oriented then if it reads from HDFS? and how does HBase access data randomly? Well, it uses a **hash table** internally, and stores the actual data in HDFS indexed in such a way that it can do fast lookups of the data from the metadata stored in the hash table. The concepts of how it stores it’s data is apparently modeled after BigTable in that there are column-families, so it’s not as structured as a relational database. Since it stores so much data in the files, it can scale a LOT- it can store petabytes of data sharded and distributed across thousands of servers and since it’s indexed it has a fairly quick lookup across all of that! If you want to learn more about the internal workings of HBase, go for it but I’ll stop here.
  
Back to Hadoop and HBase - there are also a few other tools and frameworks built on top of Hadoop, like **Ambari, Pig, Flume** and many others, but I’ll let you get in the trenches of those yourself. Now we’ll veer off from Hadoop’s MapReduce and explore another data processing engine and see what it has to offer and how it compares to Hadoop and MapReduce.

### Spark - Processing
And that brings us to Spark. The past few sections were about storage, now we’re in processing land. Spark is also an analytics engine for big data processing, just like MapReduce. Spark picks up where Hadoop/MapReduce leaves off in 2011. With Spark, let’s pick up where we left off with Hadoo’s history and then go to the technology.
With Hadoop we left off in 2011 where Hadoop 1.0 was introduced by Apache.
Spark was towards the tail end of that in 2009 in UC Berkeley  and open sourced in 2010 and moved to Apache in 2013.
This is an important starting point because it sets the context for what brought about Spark - which was to address some of the limitations of Hadoop. 

  One key difference of Spark is that it processes all data in memory, as opposed to Hadoop which writes each intermediate output of the processing step to disk. In Hadoop, this repeaded disk I/O takes it’s toll and slows down processing. Spark runs processes data up to 100x faster than Hadoop in memory. The second upgrade of Spark is that it is built as a unified analytics platform. This means that there are a lot more capabilities built on top of Spark that allows it to be used for many different things, all with the same underlying technology of Spark. Similar to Hive and Hbase, Spark has tons of technologies built on top of it for many different workloads and use-cases, like machine learning, streaming data in continously for processing and SQL-like queries. It is also well supported in that it supports many programming languages like R, python, scala and java while Hadoop is written only in Java. While Spark largely improves upon Hadoop’s MapReduce in terms of speed, performance, capabilities and support, one drawback is that it could be more expensive since it requires machines with lots of RAM to process all this data in memory.

Spark’s main component that does in-memory processing is SparkCore. It doesn’t have it’s own storage but can read from and write to HDFS or many other data sources like S3, RDBMS, elastic search, Redshift, Couchbase, Cassandra and many others. There’s many libraries that are built on top of Spark Core, like Spark SQL (similar to HQL), MLLib for machine learning, GraphX for graph problems and continuously streaming input data.

Hadoop and Spark are also complementary and many companies use them together. Hadoop has HDFS, and YARN and a mapreduce execution engine, but a hadoop job could also be deployed on a Spark, Tez or Presto distribution engines while still using HDFS and YARN to share a common cluster and datasets. This ensures consistency and unifying user, machine learning, streaming workflows all in the same place instead of having them be siloed. Some of Spark’s strengths are in interactive queries with Spark SQL, running machine learning algorithms and real-time workloads (?). 

What are all the other technologies in this Spark “ecosystem”. As we talked about, hdfs is in there. HBase is in there.

### Cassandra - Storage
it’s a wide column-store. It resembles a traditional RDBMS but provides a lot more flexibility. Rows can have different columns. It uses a query language called CQL which stands for Cassandra Query Language and it’s pretty close to SQL. It was created by facebook for the Inbox search feature and where it shines is in high availability and scalability. But it could have low consistency. That’s the tradeoff.

### Druid - Storage
It’s an analytics database. It’s column oriented. It’s generally used as a warehouse. Simple. Everything else about it is important if you’re evaluating this technology to use.
How much can it scale? it can be sharded across tens to hundreds of servers. It can ingest data at millions of records per second (that sounds ridiculously fast), and has sub-second latency. 
You can scale by adding new servers, and it’ll automatically distribute data across those clusters without any downtime!
Is it fault tolerant? Yes if a server fails, it automatically routes traffic to other servers.
Backups and restores? In case something bad happens or you want to move it to a different cluster or dbms, you’ll want to take daily backups and have a mechanism to restore. Sometimes you’ll need to build tooling around it to create this functionality, other times. Druid it turns out stores a copy of your data in “deep storage” right when you ingest it. Deep storage is generally object store, or file system store like S3 from Amazon, HDFS from Hadoop or GCS from Google.
Druid stores data in “segments” and caches it on local disk and in-mempory.So it’s not actually accessing “deep storage” during a query, and so queries can be fast.
The Druid website has a more information, and it’s relatively easy to read and digest compared to websites for other technologies that I’ve come across, so I’d recommend you check it out if you’re further interested in learning about Druid.

